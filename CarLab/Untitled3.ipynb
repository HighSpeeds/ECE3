{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f471d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carUtils import *\n",
    "from linesUtils import *\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ae8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3875f623",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c806e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA=0.9\n",
    "SEED=42\n",
    "BATCH_SIZE=40\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "env=Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6d772cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeReward(env):\n",
    "    \"\"\"\n",
    "    right now we are going to use a simple reward of the deviation from the line, well the negative of it\n",
    "    \"\"\"\n",
    "    return -env.distance_from_track()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18025f45",
   "metadata": {},
   "source": [
    "### Set up possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6ba5ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-60., -60.],\n",
       "       [-60.,  60.],\n",
       "       [ 60., -60.],\n",
       "       [ 60.,  60.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_motor_percentages=[-60,60]\n",
    "actions_to_MotorPer=np.empty((len(possible_motor_percentages)**2,2))\n",
    "for i in range(len(possible_motor_percentages)):\n",
    "    for j in range(len(possible_motor_percentages)):\n",
    "        actions_to_MotorPer[i*len(possible_motor_percentages)+j]=[possible_motor_percentages[i],\n",
    "                                                                 possible_motor_percentages[j]]\n",
    "actions_to_MotorPer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a8b762",
   "metadata": {},
   "source": [
    "## set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "071ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_output_states):\n",
    "        super(DQN, self).__init__()\n",
    "        self.f1=nn.Linear(8,n_output_states)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a7ad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(policy_net,target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d21c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions=actions_to_MotorPer.shape[0]\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "copy_params(policy_net,target_net)\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9971f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state,p_random=0.1):\n",
    "    sample = np.random.random()\n",
    "    if sample > p_random:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            #print(policy_net(state))\n",
    "            return torch.argmax(policy_net(state)).reshape((1,1))\n",
    "    else:\n",
    "        return torch.tensor([[np.random.randint(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4b829",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65278376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states=[]\n",
    "        self.next_states=[]\n",
    "        self.rewards=[]\n",
    "        self.actions=[]\n",
    "        \n",
    "    def push(self,state,action,next_state,reward):\n",
    "        self.states.append(state)\n",
    "        self.next_states.append(next_state)\n",
    "        self.rewards.append(reward)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "    def sample(self,n_samples):\n",
    "        paired=list(zip(self.states, self.next_states, self.rewards,self.actions))\n",
    "        states,next_states,rewards,actions=zip(*random.sample(paired, n_samples))\n",
    "        \n",
    "        states=torch.stack(states).to(device)\n",
    "#         next_states=torch.stack(next_states).to(device)\n",
    "        rewards=torch.stack(rewards).to(device)\n",
    "        #print(rewards.shape)\n",
    "        actions=torch.stack(actions).to(device).squeeze((-1))\n",
    "#         print(actions.shape)\n",
    "        return states,next_states,rewards,actions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "memory=Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f9bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"optimizing\")\n",
    "    states,next_states,rewards,actions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          next_states)), device=device, dtype=torch.bool)\n",
    "    print(non_final_mask.shape)\n",
    "    non_final_next_states = torch.stack([s for s in next_states\n",
    "                                                if s is not None])\n",
    "\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "#     print(states.shape)\n",
    "#     print(actions.shape)\n",
    "    state_action_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #print(non_final_next_states.shape)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + rewards\n",
    "    print(expected_state_action_values.shape)\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    raise RuntimeError\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "678bd82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[3]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9531], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[3]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9530], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9531], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9531], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9532], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9532], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9533], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9534], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9535], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9536], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9537], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9538], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9539], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9541], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9542], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9544], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9545], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9547], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9549], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 624.9097,  765.6461, 1404.8416, 1516.6538,  588.6147,  635.4636,\n",
      "         512.7900,  584.8117], device='cuda:0')\n",
      "action\n",
      "tensor([[2]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0181,  765.4277, 1403.2375, 1518.2397,  588.7819,  635.5092,\n",
      "         512.8635,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9549], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0181,  765.4277, 1403.2375, 1518.2397,  588.7819,  635.5092,\n",
      "         512.8635,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0180,  765.4279, 1403.2395, 1518.2379,  588.7817,  635.5092,\n",
      "         512.8633,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9551], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0180,  765.4279, 1403.2395, 1518.2379,  588.7817,  635.5092,\n",
      "         512.8633,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0179,  765.4282, 1403.2413, 1518.2361,  588.7815,  635.5091,\n",
      "         512.8633,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9553], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0179,  765.4282, 1403.2413, 1518.2361,  588.7815,  635.5091,\n",
      "         512.8633,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0178,  765.4284, 1403.2432, 1518.2343,  588.7813,  635.5090,\n",
      "         512.8632,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9555], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0178,  765.4284, 1403.2432, 1518.2343,  588.7813,  635.5090,\n",
      "         512.8632,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0176,  765.4286, 1403.2450, 1518.2325,  588.7811,  635.5090,\n",
      "         512.8631,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9557], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0176,  765.4286, 1403.2450, 1518.2325,  588.7811,  635.5090,\n",
      "         512.8631,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0175,  765.4289, 1403.2468, 1518.2307,  588.7809,  635.5089,\n",
      "         512.8630,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9560], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0175,  765.4289, 1403.2468, 1518.2307,  588.7809,  635.5089,\n",
      "         512.8630,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0173,  765.4292, 1403.2487, 1518.2289,  588.7807,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9562], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0173,  765.4292, 1403.2487, 1518.2289,  588.7807,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[3]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0175,  765.4289, 1403.2468, 1518.2307,  588.7809,  635.5089,\n",
      "         512.8630,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9560], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0175,  765.4289, 1403.2468, 1518.2307,  588.7809,  635.5089,\n",
      "         512.8630,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0173,  765.4292, 1403.2487, 1518.2289,  588.7807,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9562], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0173,  765.4292, 1403.2487, 1518.2289,  588.7807,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0172,  765.4294, 1403.2505, 1518.2271,  588.7805,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9564], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0172,  765.4294, 1403.2505, 1518.2271,  588.7805,  635.5089,\n",
      "         512.8629,  584.8030], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0171,  765.4297, 1403.2524, 1518.2252,  588.7803,  635.5088,\n",
      "         512.8628,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9567], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0171,  765.4297, 1403.2524, 1518.2252,  588.7803,  635.5088,\n",
      "         512.8628,  584.8031], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0170,  765.4299, 1403.2543, 1518.2234,  588.7802,  635.5087,\n",
      "         512.8627,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9570], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0170,  765.4299, 1403.2543, 1518.2234,  588.7802,  635.5087,\n",
      "         512.8627,  584.8031], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0168,  765.4302, 1403.2561, 1518.2216,  588.7800,  635.5087,\n",
      "         512.8626,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9573], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0168,  765.4302, 1403.2561, 1518.2216,  588.7800,  635.5087,\n",
      "         512.8626,  584.8031], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0167,  765.4304, 1403.2579, 1518.2197,  588.7797,  635.5086,\n",
      "         512.8625,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9575], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0167,  765.4304, 1403.2579, 1518.2197,  588.7797,  635.5086,\n",
      "         512.8625,  584.8031], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0166,  765.4307, 1403.2598, 1518.2179,  588.7795,  635.5085,\n",
      "         512.8624,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9578], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "state\n",
      "tensor([ 625.0166,  765.4307, 1403.2598, 1518.2179,  588.7795,  635.5085,\n",
      "         512.8624,  584.8031], device='cuda:0')\n",
      "action\n",
      "tensor([[0]], device='cuda:0')\n",
      "next_state\n",
      "tensor([ 625.0165,  765.4309, 1403.2616, 1518.2161,  588.7794,  635.5085,\n",
      "         512.8624,  584.8031], device='cuda:0')\n",
      "reward\n",
      "tensor([-0.9581], device='cuda:0', dtype=torch.float64)\n",
      "------------------------------\n",
      "optimizing\n",
      "torch.Size([40])\n",
      "torch.Size([40, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:921: UserWarning: Using a target size (torch.Size([40, 1, 40])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17307/3546502685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_17307/2768850846.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.move_car(0,0), device=device,\n",
    "                                     dtype=torch.float)\n",
    "    next_state = torch.tensor(env.move_car(0,0), device=device,\n",
    "                                     dtype=torch.float)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state = torch.tensor(env.move_car(*actions_to_MotorPer[action.item()]), device=device,\n",
    "                                     dtype=torch.float)\n",
    "        \n",
    "        #check if too far off track\n",
    "        done=env.off_track()\n",
    "        #check distance and compute reward\n",
    "        reward = torch.tensor([computeReward(env)], device=device)\n",
    "        \n",
    "        if done:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        print(\"state\")\n",
    "        print(state)\n",
    "        print(\"action\")\n",
    "        print(action)\n",
    "        print(\"next_state\")\n",
    "        print(next_state)\n",
    "        print(\"reward\")\n",
    "        print(reward)\n",
    "        print(\"------------------------------\")\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b398c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913faab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
