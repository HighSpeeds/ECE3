{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d589b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carUtils import *\n",
    "from linesUtils import *\n",
    "from env import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25914386",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37c6ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA=0.9\n",
    "SEED=42\n",
    "BATCH_SIZE=40\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed354d57",
   "metadata": {},
   "source": [
    "### Set up possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8706ad76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-60., -60.],\n",
       "       [-60.,  60.],\n",
       "       [ 60., -60.],\n",
       "       [ 60.,  60.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_motor_percentages=[-60,60]\n",
    "actions_to_MotorPer=np.empty((len(possible_motor_percentages)**2,2))\n",
    "for i in range(len(possible_motor_percentages)):\n",
    "    for j in range(len(possible_motor_percentages)):\n",
    "        actions_to_MotorPer[i*len(possible_motor_percentages)+j]=[possible_motor_percentages[i],\n",
    "                                                                 possible_motor_percentages[j]]\n",
    "actions_to_MotorPer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf194c0",
   "metadata": {},
   "source": [
    "## set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2f0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_output_states):\n",
    "        super(DQN, self).__init__()\n",
    "        self.f1=nn.Linear(8,n_output_states)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f5d9eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(policy_net,target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d03eb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions=actions_to_MotorPer.shape[0]\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "copy_params(policy_net,target_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cbc8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state,p_random=0.1):\n",
    "    sample = np.random.random()\n",
    "    if sample > p_random:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            #print(policy_net(state))\n",
    "            return torch.argmax(policy_net(state)).reshape((1,1))\n",
    "    else:\n",
    "        return torch.tensor([[np.random.randint(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1be710",
   "metadata": {},
   "source": [
    "### Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffdc22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states=[]\n",
    "        self.next_states=[]\n",
    "        self.rewards=[]\n",
    "        self.actions=[]\n",
    "        \n",
    "    def push(self,state,action,next_state,reward):\n",
    "        self.states.append(state)\n",
    "        self.next_state.append(next_state)\n",
    "        self.reward.append(reward)\n",
    "        self.action.append(action)\n",
    "        \n",
    "    def sample(self,n_samples):\n",
    "        paired=list(zip(self.states, self.next_state, self.rewards,self.actions))\n",
    "        states,next_states,rewards,actions=zip(*random.sample(paired, n_samples))\n",
    "        \n",
    "        states=torch.stack(states).to(device)\n",
    "#         next_states=torch.stack(next_states).to(device)\n",
    "        rewards=torch.stack(rewards).to(device)\n",
    "        actions=torch.stack(actions).to(device)\n",
    "        return states,next_states,rewards,actions\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    print(\"optimizing\")\n",
    "    states,next_states,rewards,actions = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          next_states)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in next_states\n",
    "                                                if s is not None])\n",
    "\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(states).gather(1, actions)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + rewards\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e34eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    state = torch.tensor(env.move_car(0,0), device=device,\n",
    "                                     dtype=torch.float)\n",
    "    next_state = torch.tensor(env.move_car(0,0), device=device,\n",
    "                                     dtype=torch.float)\n",
    "    \n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state = torch.tensor(env.move_car(*states_to_MotorPer[action.item()]), device=device,\n",
    "                                     dtype=torch.float)\n",
    "        \n",
    "        #check if too far off track\n",
    "        done=env.off_track()\n",
    "        #check distance and compute reward\n",
    "        reward = torch.tensor([computeReward(env)], device=device)\n",
    "        \n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        print(\"state\")\n",
    "        print(state)\n",
    "        print(\"action\")\n",
    "        print(action)\n",
    "        print(\"next_state\")\n",
    "        print(next_state)\n",
    "        print(\"reward\")\n",
    "        print(reward)\n",
    "        print(\"------------------------------\")\n",
    "        memory.push(state, action, next_state, reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
